{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PubMed Journal Article Fetcher for Google Colab\n",
        "This notebook fetches articles from specified journals for a given month using PubMed API"
      ],
      "metadata": {
        "id": "6hIkSDQe8d3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "\n",
        "!pip install biopython pandas requests\n",
        "\n",
        "import os, pandas as pd, requests, time, warnings\n",
        "from Bio import Entrez\n",
        "from datetime import datetime, timedelta\n",
        "import xml.etree.ElementTree as ET\n",
        "from typing import List, Dict, Optional\n",
        "# warnings.filterwarnings(‘ignore’)"
      ],
      "metadata": {
        "id": "rQ5dQa108bCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "668b9196-2fab-4691-d380-314ddecc5334"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions for use\n",
        "\n",
        "📋 INSTRUCTIONS:\n",
        "\n",
        "1. Update the EMAIL variable with your email address (required by NCBI)\n",
        "2. Modify the JOURNALS list with your desired journals\n",
        "3. Set the YEAR and MONTH you want to search\n",
        "4. Run the main() function\n",
        "\n",
        "⚠️  IMPORTANT NOTES:\n",
        "\n",
        "- Use your real email address - it’s required by NCBI’s usage policy\n",
        "- Journal names should match PubMed’s format exactly\n",
        "- Large queries may take several minutes to complete\n",
        "- Be respectful of API rate limits\n",
        "\n",
        "🚀 To start, run: main()"
      ],
      "metadata": {
        "id": "u_TZagk_ARly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration - MODIFY THESE VALUES\n",
        "EMAIL = os.getenv(\"UNPAYWALL_EMAIL\", \"\")\n",
        "\n",
        "JOURNALS = [\n",
        "    \"International Forum of Allergy & Rhinology\",\n",
        "    \"Rhinology\",\n",
        "    \"JAMA Otolaryngology–Head & Neck Surgery\",\n",
        "    \"Otolaryngology–Head and Neck Surgery\",\n",
        "    \"European Annals of Otorhinolaryngology–Head and Neck Diseases\",\n",
        "    \"Journal of Voice\",\n",
        "    \"American Journal of Rhinology & Allergy\",\n",
        "    \"JARO – Journal of the Association for Research in Otolaryngology\",\n",
        "    \"Journal of Otolaryngology–Head & Neck Surgery\",\n",
        "    \"Laryngoscope\",\n",
        "    \"Auris Nasus Larynx\"\n",
        "    \"new england journal of medicine\",\n",
        "    \"JAMA\"\n",
        "]"
      ],
      "metadata": {
        "id": "gzIuu6HoBSym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "YEAR = now.year\n",
        "MONTH = now.month - 1"
      ],
      "metadata": {
        "id": "c5SuSLjlpAxh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_xUOYKgy-fl"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "class PubMedFetcher:\n",
        "\n",
        "    def __init__(self, email: str):\n",
        "        \"\"\"Initialize with email for API requests\"\"\"\n",
        "        self.email = email\n",
        "        Entrez.email = email\n",
        "        # Be respectful to NCBI servers\n",
        "        self.request_delay = 0.34  # ~3 requests per second max\n",
        "\n",
        "    def search_articles(self, journals: List[str], year: int, month: int) -> List[str]:\n",
        "        \"\"\"\n",
        "        Search for articles in specified journals for given month/year\n",
        "\n",
        "        Args:\n",
        "            journals: List of journal names\n",
        "            year: Year to search\n",
        "            month: Month to search (1-12)\n",
        "\n",
        "        Returns:\n",
        "            List of PubMed IDs\n",
        "        \"\"\"\n",
        "        # Format date range for the month\n",
        "        start_date = f\"{year}/{month:02d}/01\"\n",
        "\n",
        "        # Calculate last day of month\n",
        "        if month == 12:\n",
        "            next_month = 1\n",
        "            next_year = year + 1\n",
        "        else:\n",
        "            next_month = month + 1\n",
        "            next_year = year\n",
        "\n",
        "        last_day = (datetime(next_year, next_month, 1) - timedelta(days=1)).day\n",
        "        end_date = f\"{year}/{month:02d}/{last_day}\"\n",
        "\n",
        "        # Build search query\n",
        "        journal_query = \" OR \".join([f'\"{journal}\"[Journal]' for journal in journals])\n",
        "        date_query = f\"({start_date}[PDAT] : {end_date}[PDAT])\"\n",
        "        search_query = f\"({journal_query}) AND {date_query}\"\n",
        "\n",
        "        print(f\"Search query: {search_query}\")\n",
        "        print(f\"Searching for articles from {start_date} to {end_date}\")\n",
        "\n",
        "        try:\n",
        "            # Search PubMed\n",
        "            handle = Entrez.esearch(\n",
        "                db=\"pubmed\",\n",
        "                term=search_query,\n",
        "                retmax=10000,  # Adjust based on expected results\n",
        "                sort=\"pub+date\"\n",
        "            )\n",
        "            search_results = Entrez.read(handle)\n",
        "            handle.close()\n",
        "\n",
        "            id_list = search_results[\"IdList\"]\n",
        "            print(f\"Found {len(id_list)} articles\")\n",
        "            return id_list\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error searching PubMed: {e}\")\n",
        "            return []\n",
        "\n",
        "    def fetch_article_details(self, pmid_list: List[str]) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Fetch detailed information for articles\n",
        "\n",
        "        Args:\n",
        "            pmid_list: List of PubMed IDs\n",
        "\n",
        "        Returns:\n",
        "            List of article dictionaries\n",
        "        \"\"\"\n",
        "        articles = []\n",
        "        batch_size = 200  # Process in batches to avoid overwhelming API\n",
        "\n",
        "        for i in range(0, len(pmid_list), batch_size):\n",
        "            batch = pmid_list[i:i + batch_size]\n",
        "            print(f\"Processing batch {i//batch_size + 1}/{(len(pmid_list)-1)//batch_size + 1}\")\n",
        "\n",
        "            try:\n",
        "                # Fetch article details\n",
        "                handle = Entrez.efetch(\n",
        "                    db=\"pubmed\",\n",
        "                    id=\",\".join(batch),\n",
        "                    rettype=\"xml\",\n",
        "                    retmode=\"xml\"\n",
        "                )\n",
        "                records = handle.read()\n",
        "                handle.close()\n",
        "\n",
        "                # Parse XML\n",
        "                root = ET.fromstring(records)\n",
        "\n",
        "                for article_elem in root.findall(\".//PubmedArticle\"):\n",
        "                    article_info = self._parse_article_xml(article_elem)\n",
        "                    if article_info:\n",
        "                        articles.append(article_info)\n",
        "\n",
        "                # Be respectful to servers\n",
        "                time.sleep(self.request_delay)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching batch: {e}\")\n",
        "                continue\n",
        "\n",
        "        return articles\n",
        "\n",
        "    def _parse_article_xml(self, article_elem) -> Optional[Dict]:\n",
        "        \"\"\"Parse article XML element to extract information\"\"\"\n",
        "        try:\n",
        "            # Extract basic article info\n",
        "            medline_citation = article_elem.find(\".//MedlineCitation\")\n",
        "            article = medline_citation.find(\".//Article\")\n",
        "\n",
        "            # PMID\n",
        "            pmid = medline_citation.find(\".//PMID\").text\n",
        "\n",
        "            # Title\n",
        "            title_elem = article.find(\".//ArticleTitle\")\n",
        "            title = title_elem.text if title_elem is not None else \"N/A\"\n",
        "\n",
        "            # Authors\n",
        "            authors = []\n",
        "            author_list = article.find(\".//AuthorList\")\n",
        "            if author_list is not None:\n",
        "                for author in author_list.findall(\".//Author\"):\n",
        "                    last_name = author.find(\".//LastName\")\n",
        "                    first_name = author.find(\".//ForeName\")\n",
        "                    if last_name is not None:\n",
        "                        author_name = last_name.text\n",
        "                        if first_name is not None:\n",
        "                            author_name += f\", {first_name.text}\"\n",
        "                        authors.append(author_name)\n",
        "\n",
        "            authors_str = \"; \".join(authors[:5])  # Limit to first 5 authors\n",
        "            if len(authors) > 5:\n",
        "                authors_str += \" et al.\"\n",
        "\n",
        "            # Journal\n",
        "            journal_elem = article.find(\".//Journal/Title\")\n",
        "            journal = journal_elem.text if journal_elem is not None else \"N/A\"\n",
        "\n",
        "            # Publication date\n",
        "            pub_date = article.find(\".//Journal/JournalIssue/PubDate\")\n",
        "            year = month = day = \"\"\n",
        "\n",
        "            if pub_date is not None:\n",
        "                year_elem = pub_date.find(\".//Year\")\n",
        "                month_elem = pub_date.find(\".//Month\")\n",
        "                day_elem = pub_date.find(\".//Day\")\n",
        "\n",
        "                year = year_elem.text if year_elem is not None else \"\"\n",
        "                month = month_elem.text if month_elem is not None else \"\"\n",
        "                day = day_elem.text if day_elem is not None else \"\"\n",
        "\n",
        "            pub_date_str = f\"{year}-{month}-{day}\".strip(\"-\")\n",
        "\n",
        "            # Volume and Issue\n",
        "            volume_elem = article.find(\".//Journal/JournalIssue/Volume\")\n",
        "            issue_elem = article.find(\".//Journal/JournalIssue/Issue\")\n",
        "\n",
        "            volume = volume_elem.text if volume_elem is not None else \"\"\n",
        "            issue = issue_elem.text if issue_elem is not None else \"\"\n",
        "\n",
        "            # Pages\n",
        "            pagination = article.find(\".//Pagination/MedlinePgn\")\n",
        "            pages = pagination.text if pagination is not None else \"\"\n",
        "\n",
        "            # DOI\n",
        "            article_ids = article_elem.find(\".//PubmedData/ArticleIdList\")\n",
        "            doi = \"\"\n",
        "            if article_ids is not None:\n",
        "                for article_id in article_ids.findall(\".//ArticleId\"):\n",
        "                    if article_id.get(\"IdType\") == \"doi\":\n",
        "                        doi = article_id.text\n",
        "                        break\n",
        "\n",
        "            # Abstract\n",
        "            abstract_elem = article.find(\".//Abstract/AbstractText\")\n",
        "            abstract = \"\"\n",
        "            if abstract_elem is not None:\n",
        "                # Handle structured abstracts\n",
        "                if abstract_elem.get(\"Label\"):\n",
        "                    abstract_parts = []\n",
        "                    for abs_part in article.findall(\".//Abstract/AbstractText\"):\n",
        "                        label = abs_part.get(\"Label\", \"\")\n",
        "                        text = abs_part.text or \"\"\n",
        "                        if label:\n",
        "                            abstract_parts.append(f\"{label}: {text}\")\n",
        "                        else:\n",
        "                            abstract_parts.append(text)\n",
        "                    abstract = \" \".join(abstract_parts)\n",
        "                else:\n",
        "                    abstract = abstract_elem.text or \"\"\n",
        "\n",
        "            return {\n",
        "                \"PMID\": pmid,\n",
        "                \"Title\": title,\n",
        "                \"Authors\": authors_str,\n",
        "                \"Journal\": journal,\n",
        "                \"Publication_Date\": pub_date_str,\n",
        "                \"Volume\": volume,\n",
        "                \"Issue\": issue,\n",
        "                \"Pages\": pages,\n",
        "                \"DOI\": doi,\n",
        "                \"Abstract\": abstract[:500] + \"...\" if len(abstract) > 500 else abstract  # Truncate long abstracts\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing article: {e}\")\n",
        "            return None\n",
        "\n",
        "def main(year: int, month: int): # Modified to accept year and month\n",
        "\n",
        "    print(\"=== PubMed Journal Article Fetcher ===\\n\")\n",
        "\n",
        "    # Configuration - MODIFY THESE VALUES\n",
        "    EMAIL = \"shvecht@gmail.com\"  # Replace with your email\n",
        "\n",
        "    JOURNALS = [\n",
        "        \"International Forum of Allergy & Rhinology\",\n",
        "        \"Rhinology\",\n",
        "        \"JAMA Otolaryngology–Head & Neck Surgery\",\n",
        "        \"Otolaryngology–Head and Neck Surgery\",\n",
        "        \"European Annals of Otorhinolaryngology–Head and Neck Diseases\",\n",
        "        \"Journal of Voice\",\n",
        "        \"American Journal of Rhinology & Allergy\",\n",
        "        \"JARO – Journal of the Association for Research in Otolaryngology\",\n",
        "        \"Journal of Otolaryngology–Head & Neck Surgery\",\n",
        "        \"Laryngoscope\",\n",
        "        \"Auris Nasus Larynx\", # Corrected typo here\n",
        "        \"new england journal of medicine\", # Added back the missing journals\n",
        "        \"JAMA\" # Added back the missing journals\n",
        "    ]  # Replace with your journal list\n",
        "\n",
        "    # Using the passed in year and month instead of global variables\n",
        "    search_year = year\n",
        "    search_month = month\n",
        "\n",
        "    print(f\"Email: {EMAIL}\")\n",
        "    print(f\"Journals: {', '.join(JOURNALS)}\")\n",
        "    print(f\"Search period: {search_year}-{search_month:02d}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Validate email\n",
        "    if EMAIL == \"your.email@example.com\":\n",
        "        print(\"⚠️  Please update the EMAIL variable with your actual email address!\")\n",
        "        print(\"This is required by NCBI's API usage policy.\")\n",
        "        return\n",
        "\n",
        "    # Initialize fetcher\n",
        "    fetcher = PubMedFetcher(EMAIL)\n",
        "\n",
        "    # Search for articles using the passed in year and month\n",
        "    print(\"🔍 Searching for articles...\")\n",
        "    pmid_list = fetcher.search_articles(JOURNALS, search_year, search_month)\n",
        "\n",
        "    if not pmid_list:\n",
        "        print(\"❌ No articles found matching the criteria.\")\n",
        "        return\n",
        "\n",
        "    # Fetch article details\n",
        "    print(f\"\\n📖 Fetching details for {len(pmid_list)} articles...\")\n",
        "    articles = fetcher.fetch_article_details(pmid_list)\n",
        "\n",
        "    if not articles:\n",
        "        print(\"❌ Failed to fetch article details.\")\n",
        "        return\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(articles)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n✅ Successfully retrieved {len(articles)} articles!\")\n",
        "    print(f\"\\nColumns: {', '.join(df.columns.tolist())}\")\n",
        "\n",
        "    # Show first few rows\n",
        "    print(f\"\\nFirst 5 articles:\")\n",
        "    print(df.head().to_string(max_colwidth=50))\n",
        "\n",
        "    # Save to CSV\n",
        "    output_dir = os.path.join(str(search_year), f\"{search_month:02d}\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    filename = os.path.join(output_dir, f\"pubmed_articles_{search_year}_{search_month:02d}.csv\")\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"\\n💾 Results saved to: {filename}\")\n",
        "\n",
        "    # Display summary statistics\n",
        "    print(f\"\\n📊 Summary:\")\n",
        "    print(f\"Total articles: {len(articles)}\")\n",
        "    print(f\"Unique journals: {df['Journal'].nunique()}\")\n",
        "    print(f\"Articles per journal:\")\n",
        "    journal_counts = df['Journal'].value_counts()\n",
        "    for journal, count in journal_counts.head(10).items():\n",
        "        print(f\"  • {journal}: {count}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = os.path.join(str(YEAR), f\"{MONTH:02d}\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "df = main(YEAR, MONTH)\n",
        "# df_raw should be your full all-hits table\n",
        "df.to_csv(os.path.join(output_dir, \"ent_raw_results.csv\"), index=False)\n",
        "df.to_json(os.path.join(output_dir, \"ent_all_results.json\"), orient=\"records\", force_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "xAVY_hYgpk5c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}