{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "PubMed Journal Article Fetcher for Google Colab\n",
    "This notebook fetches articles from specified journals for a given month using PubMed API"
   ],
   "metadata": {
    "id": "6hIkSDQe8d3P"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Install required packages\n\n!pip install biopython pandas requests\n\nimport os, pandas as pd, requests, time, warnings, json\nfrom Bio import Entrez\nfrom datetime import datetime, timedelta\nimport xml.etree.ElementTree as ET\nfrom typing import List, Dict, Optional\nimport calendar\n# warnings.filterwarnings('ignore')\n"
   ],
   "metadata": {
    "id": "rQ5dQa108bCT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "668b9196-2fab-4691-d380-314ddecc5334"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting biopython\n",
      "  Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: biopython\n",
      "Successfully installed biopython-1.85\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Instructions for use\n\n\ud83d\udccb INSTRUCTIONS:\n\n1. Update the EMAIL variable with your email address (required by NCBI)\n2. Adjust JOURNALS, ENT_KEYWORDS, NEGATIVE_TERMS, and LOOKBACK_DAYS to shape the search\n3. Run the cells to execute main() and harvest only new PMIDs within the rolling window or the requested month\n\n\u26a0\ufe0f  IMPORTANT NOTES:\n\n- Use your real email address - it\u2019s required by NCBI\u2019s usage policy\n- Journal names should match PubMed\u2019s format exactly\n- Large queries may take several minutes to complete\n- Be respectful of API rate limits\n- Previously harvested PMIDs are stored in data/ent_search/seen_pmids.json to avoid duplicates\n- When running via GitHub Actions, you can supply TARGET_MONTH (YYYY-MM) or YEAR + MONTH inputs to rerun a specific calendar month instead of the rolling LOOKBACK_DAYS window.\n\n\ud83d\ude80 To start, run: main()\n"
   ],
   "metadata": {
    "id": "u_TZagk_ARly"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Parameters\nEMAIL = os.getenv(\"UNPAYWALL_EMAIL\", \"\")\n\nJOURNALS = [\n    \"International Forum of Allergy & Rhinology\",\n    \"Rhinology\",\n    \"JAMA Otolaryngology\u2013Head & Neck Surgery\",\n    \"Otolaryngology\u2013Head and Neck Surgery\",\n    \"European Annals of Otorhinolaryngology\u2013Head and Neck Diseases\",\n    \"Journal of Voice\",\n    \"American Journal of Rhinology & Allergy\",\n    \"JARO \u2013 Journal of the Association for Research in Otolaryngology\",\n    \"Journal of Otolaryngology\u2013Head & Neck Surgery\",\n    \"Laryngoscope\",\n    \"Auris Nasus Larynx\",\n    \"new england journal of medicine\",\n    \"JAMA\"\n]\n\nENT_KEYWORDS = [\n    \"otolaryngology\",\n    \"ENT\",\n    \"sinus\",\n    \"nasal\",\n    \"larynx\",\n    \"otology\",\n    \"rhinology\",\n    \"head and neck surgery\"\n]\n\nNEGATIVE_TERMS = [\n    \"veterinary\",\n    \"rodent\",\n    \"mouse\",\n    \"rat\",\n    \"bovine\",\n    \"porcine\",\n    \"canine\",\n    \"feline\"\n]\n\nLOOKBACK_DAYS = int(os.getenv(\"LOOKBACK_DAYS\", \"30\"))\nTARGET_MONTH = os.getenv(\"TARGET_MONTH\", \"\").strip()\nTARGET_YEAR = os.getenv(\"TARGET_YEAR\", \"\").strip()\nTARGET_MONTH_NUMBER = os.getenv(\"TARGET_MONTH_NUMBER\", \"\").strip()\nOUTPUT_DIR = os.path.join(\"data\", \"ent_search\")\n\nALLOW_EMPTY_HARVEST = os.getenv(\"ALLOW_EMPTY_HARVEST\", \"false\").lower() == \"true\"\n"
   ],
   "metadata": {
    "id": "gzIuu6HoBSym"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "RUN_STARTED_AT = datetime.now()\n"
   ],
   "metadata": {
    "id": "c5SuSLjlpAxh"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_xUOYKgy-fl"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\ndef load_seen_pmids(path: str) -> set:\n    \"\"\"Load a set of previously seen PMIDs from disk.\"\"\"\n    if os.path.exists(path):\n        try:\n            with open(path) as f:\n                data = json.load(f)\n                if isinstance(data, list):\n                    return set(map(str, data))\n        except Exception as exc:\n            print(f\"\u26a0\ufe0f  Could not load seen PMIDs: {exc}\")\n    return set()\n\ndef save_seen_pmids(path: str, pmids: set) -> None:\n    \"\"\"Persist a set of PMIDs to disk.\"\"\"\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(sorted(pmids), f, indent=2)\n\ndef compute_requested_window(target_month: str, target_year: str, target_month_number: str):\n    \"\"\"Return a (start_date, end_date) tuple for a requested calendar month.\"\"\"\n    target_month = (target_month or \"\").strip()\n    target_year = (target_year or \"\").strip()\n    target_month_number = (target_month_number or \"\").strip()\n\n    if target_month:\n        try:\n            start_date = datetime.strptime(target_month, \"%Y-%m\")\n        except ValueError as exc:\n            raise ValueError(f\"TARGET_MONTH must be YYYY-MM; received '{target_month}'\") from exc\n        end_day = calendar.monthrange(start_date.year, start_date.month)[1]\n        end_date = start_date.replace(day=end_day, hour=23, minute=59, second=59)\n        return start_date, end_date\n\n    if target_year and target_month_number:\n        try:\n            year_int = int(target_year)\n            month_int = int(target_month_number)\n            start_date = datetime(year_int, month_int, 1)\n        except ValueError as exc:\n            raise ValueError(\n                f\"TARGET_YEAR and TARGET_MONTH_NUMBER must form a valid date; received '{target_year}-{target_month_number}'\"\n            ) from exc\n        end_day = calendar.monthrange(year_int, month_int)[1]\n        end_date = start_date.replace(day=end_day, hour=23, minute=59, second=59)\n        return start_date, end_date\n\n    return None\n\nclass PubMedFetcher:\n\n    def __init__(self, email: str):\n        \"\"\"Initialize with email for API requests\"\"\"\n        self.email = email\n        Entrez.email = email\n        # Be respectful to NCBI servers\n        self.request_delay = 0.34  # ~3 requests per second max\n\n    def search_articles(\n        self,\n        journals: List[str],\n        keywords: List[str],\n        negative_terms: List[str],\n        lookback_days: int,\n        start_date_override: Optional[datetime] = None,\n        end_date_override: Optional[datetime] = None,\n    ):\n        \"\"\"\n        Search for articles in specified journals with keyword filters within a rolling or fixed window.\n\n        Args:\n            journals: List of journal names\n            keywords: Positive keyword filters\n            negative_terms: Terms to exclude from results\n            lookback_days: Number of days to include in the rolling window\n            start_date_override: Explicit start date to use instead of the rolling window\n            end_date_override: Explicit end date to use instead of the rolling window\n\n        Returns:\n            Tuple of (List of PubMed IDs, start_date_str, end_date_str)\n        \"\"\"\n        end_date = end_date_override or datetime.now()\n        start_date = start_date_override or end_date - timedelta(days=lookback_days)\n\n        start_date_str = start_date.strftime(\"%Y/%m/%d\")\n        end_date_str = end_date.strftime(\"%Y/%m/%d\")\n\n        journal_query = \" OR \".join([f'\"{journal}\"[Journal]' for journal in journals]) if journals else \"\"\n        keyword_query = \" OR \".join([f'\"{kw}\"[Title/Abstract]' for kw in keywords]) if keywords else \"\"\n        negative_query = \" AND \".join([f'NOT \"{term}\"[Title/Abstract]' for term in negative_terms]) if negative_terms else \"\"\n\n        query_parts = []\n        if journal_query:\n            query_parts.append(f\"({journal_query})\")\n        if keyword_query:\n            query_parts.append(f\"({keyword_query})\")\n        if negative_query:\n            query_parts.append(negative_query)\n        query_parts.append(f\"({start_date_str}[PDAT] : {end_date_str}[PDAT])\")\n\n        final_query = \" AND \".join(query_parts)\n\n        try:\n            print(f\"Querying PubMed with: {final_query}\")\n            handle = Entrez.esearch(db=\"pubmed\", term=final_query, datetype=\"pdat\", retmax=100000)\n            record = Entrez.read(handle)\n            handle.close()\n            time.sleep(self.request_delay)\n\n            search_results = record\n            id_list = search_results[\"IdList\"]\n            print(f\"Found {len(id_list)} articles\")\n            return id_list, start_date_str, end_date_str\n\n        except Exception as e:\n            print(f\"Error searching PubMed: {e}\")\n            return [], start_date_str, end_date_str\n\n    def fetch_article_details(self, pmid_list: List[str]) -> List[Dict]:\n        \"\"\"\n        Fetch detailed information for articles\n\n        Args:\n            pmid_list: List of PubMed IDs\n\n        Returns:\n            List of article dictionaries\n        \"\"\"\n        articles = []\n        batch_size = 200  # Process in batches to avoid overwhelming API\n\n        for i in range(0, len(pmid_list), batch_size):\n            batch = pmid_list[i:i + batch_size]\n            print(f\"Fetching batch {i//batch_size + 1} with {len(batch)} articles\")\n            try:\n                handle = Entrez.efetch(db=\"pubmed\", id=batch, rettype=\"xml\", retmode=\"text\")\n                records = Entrez.read(handle)\n                handle.close()\n                time.sleep(self.request_delay)\n\n                for article in records.get(\"PubmedArticle\", []):\n                    parsed_article = self.parse_article(article)\n                    if parsed_article:\n                        articles.append(parsed_article)\n\n            except Exception as e:\n                print(f\"Error fetching details for batch {i//batch_size + 1}: {e}\")\n                continue\n\n        return articles\n\n    def parse_article(self, article) -> Optional[Dict]:\n        \"\"\"\n        Parse article details from PubMed XML\n        \"\"\"\n        try:\n            journal = article.find(\".//Journal/Title\").text or \"N/A\"\n            title_elem = article.find(\".//ArticleTitle\")\n            title = title_elem.text if title_elem is not None else \"N/A\"\n\n            pmid_elem = article.find(\".//PMID\")\n            pmid = pmid_elem.text if pmid_elem is not None else \"\"\n\n            authors_list = article.findall(\".//Author\")\n            authors = []\n            for author in authors_list:\n                last = author.findtext(\"LastName\", default=\"\")\n                fore = author.findtext(\"ForeName\", default=\"\")\n                full_name = \" \".join([fore, last]).strip()\n                if full_name:\n                    authors.append(full_name)\n            authors_str = \", \".join(authors)\n\n            pub_date_elem = article.find(\".//PubDate\")\n            year = pub_date_elem.findtext(\"Year\", default=\"N/A\") if pub_date_elem is not None else \"N/A\"\n            month = pub_date_elem.findtext(\"Month\", default=\"N/A\") if pub_date_elem is not None else \"N/A\"\n            day = pub_date_elem.findtext(\"Day\", default=\"\") if pub_date_elem is not None else \"\"\n            pub_date_str = f\"{year}/{month}/{day}\" if day else f\"{year}/{month}\"\n\n            doi_elem = article.find(\".//ArticleId[@IdType='doi']\")\n            doi = doi_elem.text if doi_elem is not None else \"N/A\"\n\n            volume = article.findtext(\".//JournalIssue/Volume\", default=\"\") or \"N/A\"\n            issue = article.findtext(\".//JournalIssue/Issue\", default=\"\") or \"N/A\"\n            pages = article.findtext(\".//Pagination/MedlinePgn\", default=\"\") or \"N/A\"\n\n            abstract_elem = article.find(\".//Abstract/AbstractText\")\n            if abstract_elem is not None:\n                if list(article.findall(\".//Abstract/AbstractText\")):\n                    abstract_parts = []\n                    for abs_part in article.findall(\".//Abstract/AbstractText\"):\n                        label = abs_part.get(\"Label\", \"\")\n                        text = abs_part.text or \"\"\n                        if label:\n                            abstract_parts.append(f\"{label}: {text}\")\n                        else:\n                            abstract_parts.append(text)\n                    abstract = \" \".join(abstract_parts)\n                else:\n                    abstract = abstract_elem.text or \"\"\n            else:\n                abstract = \"\"\n\n            return {\n                \"PMID\": pmid,\n                \"Title\": title,\n                \"Authors\": authors_str,\n                \"Journal\": journal,\n                \"Publication_Date\": pub_date_str,\n                \"Volume\": volume,\n                \"Issue\": issue,\n                \"Pages\": pages,\n                \"DOI\": doi,\n                \"Abstract\": abstract[:500] + \"...\" if len(abstract) > 500 else abstract  # Truncate long abstracts\n            }\n\n        except Exception as e:\n            print(f\"Error parsing article: {e}\")\n            return None\n\ndef main():\n\n    print(\"=== PubMed Journal Article Fetcher ===\")\n\n    print(f\"Email: {EMAIL}\")\n    print(f\"Journals: {', '.join(JOURNALS)}\")\n    print(f\"Keyword filters: {', '.join(ENT_KEYWORDS)}\")\n    print(f\"Excluding terms: {', '.join(NEGATIVE_TERMS)}\")\n\n    requested_window = compute_requested_window(TARGET_MONTH, TARGET_YEAR, TARGET_MONTH_NUMBER)\n    if requested_window:\n        start_dt, end_dt = requested_window\n        print(\n            f\"Requested month window: {start_dt.strftime('%Y-%m-%d')} to {end_dt.strftime('%Y-%m-%d')} (from TARGET_MONTH/TARGET_YEAR+TARGET_MONTH_NUMBER)\"\n        )\n    else:\n        end_dt = datetime.now()\n        start_dt = end_dt - timedelta(days=LOOKBACK_DAYS)\n        print(f\"Rolling window: last {LOOKBACK_DAYS} days ({start_dt.strftime('%Y-%m-%d')} to {end_dt.strftime('%Y-%m-%d')})\")\n    print(\"-\" * 50)\n\n    # Validate email\n    if EMAIL == \"your.email@example.com\":\n        print(\"\u26a0\ufe0f  Please update the EMAIL variable with your actual email address!\")\n        print(\"This is required by NCBI's API usage policy.\")\n        return\n\n    # Initialize fetcher\n    fetcher = PubMedFetcher(EMAIL)\n\n    # Search for articles within the requested window\n    print(\"\ud83d\udd0d Searching for articles...\")\n    pmid_list, start_date, end_date = fetcher.search_articles(\n        JOURNALS,\n        ENT_KEYWORDS,\n        NEGATIVE_TERMS,\n        LOOKBACK_DAYS,\n        start_dt,\n        end_dt,\n    )\n\n    if not pmid_list:\n        print(\"\u274c No articles found matching the criteria.\")\n        print(f\"Summary: discovered {len(pmid_list)} PMIDs between {start_date} and {end_date}.\")\n        if ALLOW_EMPTY_HARVEST:\n            print(\"\u26a0\ufe0f  Empty harvest allowed via ALLOW_EMPTY_HARVEST flag; exiting without failure.\")\n            return\n        raise RuntimeError(\"No articles found for the configured search window.\")\n\n    seen_pmids_path = os.path.join(OUTPUT_DIR, \"seen_pmids.json\")\n    seen_pmids = load_seen_pmids(seen_pmids_path)\n    if seen_pmids:\n        print(f\"Loaded {len(seen_pmids)} previously harvested PMIDs.\")\n\n    new_pmids = [pmid for pmid in pmid_list if pmid not in seen_pmids]\n    print(f\"PMIDs to fetch after filtering seen set: {len(new_pmids)}\")\n\n    if not new_pmids:\n        print(\"\u26a0\ufe0f  No new PMIDs to process within this window.\")\n        print(f\"Summary: discovered {len(pmid_list)} PMIDs, seen {len(seen_pmids)}, new {len(new_pmids)} between {start_date} and {end_date}.\")\n        if ALLOW_EMPTY_HARVEST:\n            print(\"\u26a0\ufe0f  Empty harvest allowed via ALLOW_EMPTY_HARVEST flag; exiting without failure.\")\n            return\n        raise RuntimeError(\"No new PMIDs to process after filtering seen set.\")\n\n    # Fetch article details\n    print(f\"\n\ud83d\udcd6 Fetching details for {len(new_pmids)} new articles...\")\n    articles = fetcher.fetch_article_details(new_pmids)\n\n    if not articles:\n        print(\"\u274c Failed to fetch article details.\")\n        return\n\n    # Create DataFrame\n    df = pd.DataFrame(articles)\n\n    # Display results\n    print(f\"\n\u2705 Successfully retrieved {len(articles)} articles!\")\n    print(f\"\nColumns: {', '.join(df.columns.tolist())}\")\n\n    # Show first few rows\n    print(f\"\nFirst 5 articles:\")\n    print(df.head().to_string(max_colwidth=50))\n\n    # Save outputs\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    window_label = f\"{start_date.replace('/', '-')}_to_{end_date.replace('/', '-')}\"\n    csv_path = os.path.join(OUTPUT_DIR, f\"ent_raw_results_{window_label}.csv\")\n    df.to_csv(csv_path, index=False)\n\n    updated_seen_pmids = seen_pmids | set(new_pmids)\n    save_seen_pmids(seen_pmids_path, updated_seen_pmids)\n    print(f\"Updated seen PMIDs saved to: {seen_pmids_path}\")\n\n    json_path = os.path.join(OUTPUT_DIR, \"ent_all_results.json\")\n    df.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n\n    # Display summary statistics\n    print(f\"\n\ud83d\udcca Summary:\")\n    print(f\"Total new articles: {len(articles)}\")\n    print(f\"Unique journals: {df['Journal'].nunique()}\")\n    print(f\"Articles per journal:\")\n    journal_counts = df['Journal'].value_counts()\n    for journal, count in journal_counts.head(10).items():\n        print(f\"  \u2022 {journal}: {count}\")\n\n    print(f\"\n\ud83d\udcbe Raw CSV saved to: {csv_path}\")\n    print(f\"\ud83d\udcbe ENT results JSON saved to: {json_path}\")\n\n    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "df = main()\n",
    "if df is not None:\n",
    "    print(f\"\\nSaved results to {OUTPUT_DIR}\")\n"
   ],
   "metadata": {
    "id": "xAVY_hYgpk5c"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}