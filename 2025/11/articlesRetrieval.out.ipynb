{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd04633f",
   "metadata": {
    "id": "6hIkSDQe8d3P",
    "papermill": {
     "duration": 0.002109,
     "end_time": "2025-12-03T09:46:01.087811",
     "exception": false,
     "start_time": "2025-12-03T09:46:01.085702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "PubMed Journal Article Fetcher for Google Colab\n",
    "This notebook fetches articles from specified journals for a given month using PubMed API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389e6416",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-03T09:46:01.092114Z",
     "iopub.status.busy": "2025-12-03T09:46:01.091844Z",
     "iopub.status.idle": "2025-12-03T09:46:03.525226Z",
     "shell.execute_reply": "2025-12-03T09:46:03.524386Z"
    },
    "id": "rQ5dQa108bCT",
    "outputId": "668b9196-2fab-4691-d380-314ddecc5334",
    "papermill": {
     "duration": 2.436792,
     "end_time": "2025-12-03T09:46:03.526228",
     "exception": false,
     "start_time": "2025-12-03T09:46:01.089436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading biopython-1.86-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: pandas in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (2.3.3)\r\n",
      "Requirement already satisfied: requests in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (2.32.5)\r\n",
      "Requirement already satisfied: numpy in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from biopython) (2.3.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from requests) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from requests) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from requests) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from requests) (2025.11.12)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading biopython-1.86-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\r\n",
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: biopython\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed biopython-1.86\r\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "\n",
    "!pip install biopython pandas requests\n",
    "\n",
    "import os, pandas as pd, requests, time, warnings, json\n",
    "from Bio import Entrez\n",
    "from datetime import datetime, timedelta\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict, Optional\n",
    "# warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd706fd",
   "metadata": {
    "id": "u_TZagk_ARly",
    "papermill": {
     "duration": 0.001749,
     "end_time": "2025-12-03T09:46:03.529975",
     "exception": false,
     "start_time": "2025-12-03T09:46:03.528226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Instructions for use\n",
    "\n",
    "üìã INSTRUCTIONS:\n",
    "\n",
    "1. Update the EMAIL variable with your email address (required by NCBI)\n",
    "2. Adjust JOURNALS, ENT_KEYWORDS, NEGATIVE_TERMS, and LOOKBACK_DAYS to shape the search\n",
    "3. Run the cells to execute main() and harvest only new PMIDs within the rolling window\n",
    "\n",
    "‚ö†Ô∏è  IMPORTANT NOTES:\n",
    "\n",
    "- Use your real email address - it‚Äôs required by NCBI‚Äôs usage policy\n",
    "- Journal names should match PubMed‚Äôs format exactly\n",
    "- Large queries may take several minutes to complete\n",
    "- Be respectful of API rate limits\n",
    "- Previously harvested PMIDs are stored in data/ent_search/seen_pmids.json to avoid duplicates\n",
    "\n",
    "üöÄ To start, run: main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4423c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T09:46:03.534407Z",
     "iopub.status.busy": "2025-12-03T09:46:03.534127Z",
     "iopub.status.idle": "2025-12-03T09:46:03.538705Z",
     "shell.execute_reply": "2025-12-03T09:46:03.537884Z"
    },
    "id": "gzIuu6HoBSym",
    "papermill": {
     "duration": 0.007909,
     "end_time": "2025-12-03T09:46:03.539516",
     "exception": false,
     "start_time": "2025-12-03T09:46:03.531607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration - MODIFY THESE VALUES\n",
    "EMAIL = os.getenv(\"UNPAYWALL_EMAIL\", \"\")\n",
    "\n",
    "JOURNALS = [\n",
    "    \"International Forum of Allergy & Rhinology\",\n",
    "    \"Rhinology\",\n",
    "    \"JAMA Otolaryngology‚ÄìHead & Neck Surgery\",\n",
    "    \"Otolaryngology‚ÄìHead and Neck Surgery\",\n",
    "    \"European Annals of Otorhinolaryngology‚ÄìHead and Neck Diseases\",\n",
    "    \"Journal of Voice\",\n",
    "    \"American Journal of Rhinology & Allergy\",\n",
    "    \"JARO ‚Äì Journal of the Association for Research in Otolaryngology\",\n",
    "    \"Journal of Otolaryngology‚ÄìHead & Neck Surgery\",\n",
    "    \"Laryngoscope\",\n",
    "    \"Auris Nasus Larynx\",\n",
    "    \"new england journal of medicine\",\n",
    "    \"JAMA\"\n",
    "]\n",
    "\n",
    "ENT_KEYWORDS = [\n",
    "    \"otolaryngology\",\n",
    "    \"ENT\",\n",
    "    \"sinus\",\n",
    "    \"nasal\",\n",
    "    \"larynx\",\n",
    "    \"otology\",\n",
    "    \"rhinology\",\n",
    "    \"head and neck surgery\"\n",
    "]\n",
    "\n",
    "NEGATIVE_TERMS = [\n",
    "    \"veterinary\",\n",
    "    \"rodent\",\n",
    "    \"mouse\",\n",
    "    \"rat\",\n",
    "    \"bovine\",\n",
    "    \"porcine\",\n",
    "    \"canine\",\n",
    "    \"feline\"\n",
    "]\n",
    "\n",
    "LOOKBACK_DAYS = 30\n",
    "OUTPUT_DIR = os.path.join(\"data\", \"ent_search\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5bb53fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T09:46:03.543789Z",
     "iopub.status.busy": "2025-12-03T09:46:03.543621Z",
     "iopub.status.idle": "2025-12-03T09:46:03.546512Z",
     "shell.execute_reply": "2025-12-03T09:46:03.545793Z"
    },
    "id": "c5SuSLjlpAxh",
    "papermill": {
     "duration": 0.005848,
     "end_time": "2025-12-03T09:46:03.547086",
     "exception": false,
     "start_time": "2025-12-03T09:46:03.541238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "RUN_STARTED_AT = datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17516fa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T09:46:03.551664Z",
     "iopub.status.busy": "2025-12-03T09:46:03.551493Z",
     "iopub.status.idle": "2025-12-03T09:46:03.574510Z",
     "shell.execute_reply": "2025-12-03T09:46:03.573624Z"
    },
    "id": "i_xUOYKgy-fl",
    "papermill": {
     "duration": 0.026329,
     "end_time": "2025-12-03T09:46:03.575117",
     "exception": false,
     "start_time": "2025-12-03T09:46:03.548788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "def load_seen_pmids(path: str) -> set:\n",
    "    \"\"\"Load a set of previously seen PMIDs from disk.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    return set(map(str, data))\n",
    "        except Exception as exc:\n",
    "            print(f\"‚ö†Ô∏è  Could not load seen PMIDs: {exc}\")\n",
    "    return set()\n",
    "\n",
    "def save_seen_pmids(path: str, pmids: set) -> None:\n",
    "    \"\"\"Persist a set of PMIDs to disk.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(sorted(pmids), f, indent=2)\n",
    "\n",
    "class PubMedFetcher:\n",
    "\n",
    "    def __init__(self, email: str):\n",
    "        \"\"\"Initialize with email for API requests\"\"\"\n",
    "        self.email = email\n",
    "        Entrez.email = email\n",
    "        # Be respectful to NCBI servers\n",
    "        self.request_delay = 0.34  # ~3 requests per second max\n",
    "\n",
    "    def search_articles(self, journals: List[str], keywords: List[str], negative_terms: List[str], lookback_days: int):\n",
    "        \"\"\"\n",
    "        Search for articles in specified journals with keyword filters within a rolling window.\n",
    "\n",
    "        Args:\n",
    "            journals: List of journal names\n",
    "            keywords: Positive keyword filters\n",
    "            negative_terms: Terms to exclude from results\n",
    "            lookback_days: Number of days to include in the rolling window\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (List of PubMed IDs, start_date_str, end_date_str)\n",
    "        \"\"\"\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=lookback_days)\n",
    "\n",
    "        start_date_str = start_date.strftime(\"%Y/%m/%d\")\n",
    "        end_date_str = end_date.strftime(\"%Y/%m/%d\")\n",
    "\n",
    "        journal_query = \" OR \".join([f'\"{journal}\"[Journal]' for journal in journals]) if journals else \"\"\n",
    "        keyword_query = \" OR \".join([f'\"{kw}\"[Title/Abstract]' for kw in keywords]) if keywords else \"\"\n",
    "        negative_query = \" AND \".join([f'NOT \"{term}\"[Title/Abstract]' for term in negative_terms]) if negative_terms else \"\"\n",
    "\n",
    "        query_parts = []\n",
    "        if journal_query:\n",
    "            query_parts.append(f\"({journal_query})\")\n",
    "        if keyword_query:\n",
    "            query_parts.append(f\"({keyword_query})\")\n",
    "        if negative_query:\n",
    "            query_parts.append(negative_query)\n",
    "        query_parts.append(f\"({start_date_str}[PDAT] : {end_date_str}[PDAT])\")\n",
    "\n",
    "        search_query = \" AND \".join(query_parts)\n",
    "\n",
    "        print(f\"Search query: {search_query}\")\n",
    "        print(f\"Searching for articles from {start_date_str} to {end_date_str}\")\n",
    "\n",
    "        try:\n",
    "            # Search PubMed\n",
    "            handle = Entrez.esearch(\n",
    "                db=\"pubmed\",\n",
    "                term=search_query,\n",
    "                retmax=10000,  # Adjust based on expected results\n",
    "                sort=\"pub+date\",\n",
    "            )\n",
    "            search_results = Entrez.read(handle)\n",
    "            handle.close()\n",
    "\n",
    "            id_list = search_results[\"IdList\"]\n",
    "            print(f\"Found {len(id_list)} articles\")\n",
    "            return id_list, start_date_str, end_date_str\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching PubMed: {e}\")\n",
    "            return [], start_date_str, end_date_str\n",
    "\n",
    "    def fetch_article_details(self, pmid_list: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetch detailed information for articles\n",
    "\n",
    "        Args:\n",
    "            pmid_list: List of PubMed IDs\n",
    "\n",
    "        Returns:\n",
    "            List of article dictionaries\n",
    "        \"\"\"\n",
    "        articles = []\n",
    "        batch_size = 200  # Process in batches to avoid overwhelming API\n",
    "\n",
    "        for i in range(0, len(pmid_list), batch_size):\n",
    "            batch = pmid_list[i:i + batch_size]\n",
    "            print(f\"Processing batch {i//batch_size + 1}/{(len(pmid_list)-1)//batch_size + 1}\")\n",
    "\n",
    "            try:\n",
    "                # Fetch article details\n",
    "                handle = Entrez.efetch(\n",
    "                    db=\"pubmed\",\n",
    "                    id=\",\".join(batch),\n",
    "                    rettype=\"xml\",\n",
    "                    retmode=\"xml\"\n",
    "                )\n",
    "                records = handle.read()\n",
    "                handle.close()\n",
    "\n",
    "                # Parse XML\n",
    "                root = ET.fromstring(records)\n",
    "\n",
    "                for article_elem in root.findall(\".//PubmedArticle\"):\n",
    "                    article_info = self._parse_article_xml(article_elem)\n",
    "                    if article_info:\n",
    "                        articles.append(article_info)\n",
    "\n",
    "                # Be respectful to servers\n",
    "                time.sleep(self.request_delay)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching batch: {e}\")\n",
    "                continue\n",
    "\n",
    "        return articles\n",
    "\n",
    "    def _parse_article_xml(self, article_elem) -> Optional[Dict]:\n",
    "        \"\"\"Parse article XML element to extract information\"\"\"\n",
    "        try:\n",
    "            # Extract basic article info\n",
    "            medline_citation = article_elem.find(\".//MedlineCitation\")\n",
    "            article = medline_citation.find(\".//Article\")\n",
    "\n",
    "            # PMID\n",
    "            pmid = medline_citation.find(\".//PMID\").text\n",
    "\n",
    "            # Title\n",
    "            title_elem = article.find(\".//ArticleTitle\")\n",
    "            title = title_elem.text if title_elem is not None else \"\"\n",
    "\n",
    "            # Authors\n",
    "            authors = []\n",
    "            for author in article.findall(\".//Author\"):\n",
    "                last_name = author.findtext(\"LastName\", default=\"\")\n",
    "                fore_name = author.findtext(\"ForeName\", default=\"\")\n",
    "                if last_name or fore_name:\n",
    "                    authors.append(f\"{fore_name} {last_name}\".strip())\n",
    "            authors_str = \", \".join(authors) if authors else \"\"\n",
    "\n",
    "            # Journal\n",
    "            journal = article.findtext(\".//Journal/Title\", default=\"\")\n",
    "\n",
    "            # Publication Date\n",
    "            pub_date = article.find(\".//JournalIssue/PubDate\")\n",
    "            year = pub_date.findtext(\"Year\") if pub_date is not None else None\n",
    "            month = pub_date.findtext(\"Month\") if pub_date is not None else None\n",
    "            day = pub_date.findtext(\"Day\") if pub_date is not None else None\n",
    "\n",
    "            if year and month and day:\n",
    "                pub_date_str = f\"{year}-{month}-{day}\"\n",
    "            elif year and month:\n",
    "                pub_date_str = f\"{year}-{month}\"\n",
    "            elif year:\n",
    "                pub_date_str = year\n",
    "            else:\n",
    "                pub_date_str = \"\"\n",
    "\n",
    "            # Volume, Issue, Pages\n",
    "            volume = article.findtext(\".//JournalIssue/Volume\", default=\"\")\n",
    "            issue = article.findtext(\".//JournalIssue/Issue\", default=\"\")\n",
    "            pages = article.findtext(\".//Pagination/MedlinePgn\", default=\"\")\n",
    "\n",
    "            # DOI\n",
    "            doi_elem = article.find(\".//ArticleId[@IdType='doi']\")\n",
    "            doi = doi_elem.text if doi_elem is not None else \"\"\n",
    "\n",
    "            # Abstract (handle structured abstracts)\n",
    "            abstract_elem = article.find(\".//Abstract/AbstractText\")\n",
    "            abstract = \"\"\n",
    "            if abstract_elem is not None:\n",
    "                if abstract_elem.get(\"Label\"):\n",
    "                    abstract_parts = []\n",
    "                    for abs_part in article.findall(\".//Abstract/AbstractText\"):\n",
    "                        label = abs_part.get(\"Label\", \"\")\n",
    "                        text = abs_part.text or \"\"\n",
    "                        if label:\n",
    "                            abstract_parts.append(f\"{label}: {text}\")\n",
    "                        else:\n",
    "                            abstract_parts.append(text)\n",
    "                    abstract = \" \".join(abstract_parts)\n",
    "                else:\n",
    "                    abstract = abstract_elem.text or \"\"\n",
    "\n",
    "            return {\n",
    "                \"PMID\": pmid,\n",
    "                \"Title\": title,\n",
    "                \"Authors\": authors_str,\n",
    "                \"Journal\": journal,\n",
    "                \"Publication_Date\": pub_date_str,\n",
    "                \"Volume\": volume,\n",
    "                \"Issue\": issue,\n",
    "                \"Pages\": pages,\n",
    "                \"DOI\": doi,\n",
    "                \"Abstract\": abstract[:500] + \"...\" if len(abstract) > 500 else abstract  # Truncate long abstracts\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing article: {e}\")\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(\"=== PubMed Journal Article Fetcher ===\")\n",
    "\n",
    "    print(f\"Email: {EMAIL}\")\n",
    "    print(f\"Journals: {', '.join(JOURNALS)}\")\n",
    "    print(f\"Keyword filters: {', '.join(ENT_KEYWORDS)}\")\n",
    "    print(f\"Excluding terms: {', '.join(NEGATIVE_TERMS)}\")\n",
    "    print(f\"Rolling window: last {LOOKBACK_DAYS} days\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Validate email\n",
    "    if EMAIL == \"your.email@example.com\":\n",
    "        print(\"‚ö†Ô∏è  Please update the EMAIL variable with your actual email address!\")\n",
    "        print(\"This is required by NCBI's API usage policy.\")\n",
    "        return\n",
    "\n",
    "    # Initialize fetcher\n",
    "    fetcher = PubMedFetcher(EMAIL)\n",
    "\n",
    "    # Search for articles within the rolling window\n",
    "    print(\"üîç Searching for articles...\")\n",
    "    pmid_list, start_date, end_date = fetcher.search_articles(JOURNALS, ENT_KEYWORDS, NEGATIVE_TERMS, LOOKBACK_DAYS)\n",
    "\n",
    "    if not pmid_list:\n",
    "        print(\"‚ùå No articles found matching the criteria.\")\n",
    "        return\n",
    "\n",
    "    seen_pmids_path = os.path.join(OUTPUT_DIR, \"seen_pmids.json\")\n",
    "    seen_pmids = load_seen_pmids(seen_pmids_path)\n",
    "    if seen_pmids:\n",
    "        print(f\"Loaded {len(seen_pmids)} previously harvested PMIDs.\")\n",
    "\n",
    "    new_pmids = [pmid for pmid in pmid_list if pmid not in seen_pmids]\n",
    "    print(f\"PMIDs to fetch after filtering seen set: {len(new_pmids)}\")\n",
    "\n",
    "    if not new_pmids:\n",
    "        print(\"‚úÖ No new PMIDs to process within this window.\")\n",
    "        return\n",
    "\n",
    "    # Fetch article details\n",
    "    print(f\"\\nüìñ Fetching details for {len(new_pmids)} new articles...\")\n",
    "    articles = fetcher.fetch_article_details(new_pmids)\n",
    "\n",
    "    if not articles:\n",
    "        print(\"‚ùå Failed to fetch article details.\")\n",
    "        return\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\n‚úÖ Successfully retrieved {len(articles)} articles!\")\n",
    "    print(f\"\\nColumns: {', '.join(df.columns.tolist())}\")\n",
    "\n",
    "    # Show first few rows\n",
    "    print(f\"\\nFirst 5 articles:\")\n",
    "    print(df.head().to_string(max_colwidth=50))\n",
    "\n",
    "    # Save outputs\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    window_label = f\"{start_date.replace('/', '-')}_to_{end_date.replace('/', '-')}\"\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"ent_raw_results_{window_label}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    updated_seen_pmids = seen_pmids | set(new_pmids)\n",
    "    save_seen_pmids(seen_pmids_path, updated_seen_pmids)\n",
    "    print(f\"Updated seen PMIDs saved to: {seen_pmids_path}\")\n",
    "\n",
    "    json_path = os.path.join(OUTPUT_DIR, \"ent_all_results.json\")\n",
    "    df.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
    "\n",
    "    # Display summary statistics\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"Total new articles: {len(articles)}\")\n",
    "    print(f\"Unique journals: {df['Journal'].nunique()}\")\n",
    "    print(f\"Articles per journal:\")\n",
    "    journal_counts = df['Journal'].value_counts()\n",
    "    for journal, count in journal_counts.head(10).items():\n",
    "        print(f\"  ‚Ä¢ {journal}: {count}\")\n",
    "\n",
    "    print(f\"\\nüíæ Raw CSV saved to: {csv_path}\")\n",
    "    print(f\"üíæ ENT results JSON saved to: {json_path}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9db302a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T09:46:03.579973Z",
     "iopub.status.busy": "2025-12-03T09:46:03.579747Z",
     "iopub.status.idle": "2025-12-03T09:46:03.995678Z",
     "shell.execute_reply": "2025-12-03T09:46:03.994949Z"
    },
    "id": "xAVY_hYgpk5c",
    "papermill": {
     "duration": 0.419305,
     "end_time": "2025-12-03T09:46:03.996443",
     "exception": false,
     "start_time": "2025-12-03T09:46:03.577138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PubMed Journal Article Fetcher ===\n",
      "Email: shvecht@gmail.com\n",
      "Journals: International Forum of Allergy & Rhinology, Rhinology, JAMA Otolaryngology‚ÄìHead & Neck Surgery, Otolaryngology‚ÄìHead and Neck Surgery, European Annals of Otorhinolaryngology‚ÄìHead and Neck Diseases, Journal of Voice, American Journal of Rhinology & Allergy, JARO ‚Äì Journal of the Association for Research in Otolaryngology, Journal of Otolaryngology‚ÄìHead & Neck Surgery, Laryngoscope, Auris Nasus Larynx, new england journal of medicine, JAMA\n",
      "Keyword filters: otolaryngology, ENT, sinus, nasal, larynx, otology, rhinology, head and neck surgery\n",
      "Excluding terms: veterinary, rodent, mouse, rat, bovine, porcine, canine, feline\n",
      "Rolling window: last 30 days\n",
      "--------------------------------------------------\n",
      "üîç Searching for articles...\n",
      "Search query: (\"International Forum of Allergy & Rhinology\"[Journal] OR \"Rhinology\"[Journal] OR \"JAMA Otolaryngology‚ÄìHead & Neck Surgery\"[Journal] OR \"Otolaryngology‚ÄìHead and Neck Surgery\"[Journal] OR \"European Annals of Otorhinolaryngology‚ÄìHead and Neck Diseases\"[Journal] OR \"Journal of Voice\"[Journal] OR \"American Journal of Rhinology & Allergy\"[Journal] OR \"JARO ‚Äì Journal of the Association for Research in Otolaryngology\"[Journal] OR \"Journal of Otolaryngology‚ÄìHead & Neck Surgery\"[Journal] OR \"Laryngoscope\"[Journal] OR \"Auris Nasus Larynx\"[Journal] OR \"new england journal of medicine\"[Journal] OR \"JAMA\"[Journal]) AND (\"otolaryngology\"[Title/Abstract] OR \"ENT\"[Title/Abstract] OR \"sinus\"[Title/Abstract] OR \"nasal\"[Title/Abstract] OR \"larynx\"[Title/Abstract] OR \"otology\"[Title/Abstract] OR \"rhinology\"[Title/Abstract] OR \"head and neck surgery\"[Title/Abstract]) AND NOT \"veterinary\"[Title/Abstract] AND NOT \"rodent\"[Title/Abstract] AND NOT \"mouse\"[Title/Abstract] AND NOT \"rat\"[Title/Abstract] AND NOT \"bovine\"[Title/Abstract] AND NOT \"porcine\"[Title/Abstract] AND NOT \"canine\"[Title/Abstract] AND NOT \"feline\"[Title/Abstract] AND (2025/11/03[PDAT] : 2025/12/03[PDAT])\n",
      "Searching for articles from 2025/11/03 to 2025/12/03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 articles\n",
      "‚ùå No articles found matching the criteria.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "df = main()\n",
    "if df is not None:\n",
    "    print(f\"\\nSaved results to {OUTPUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.45259,
   "end_time": "2025-12-03T09:46:04.213511",
   "environment_variables": {},
   "exception": null,
   "input_path": "articlesRetrieval.ipynb",
   "output_path": "articlesRetrieval.out.ipynb",
   "parameters": {},
   "start_time": "2025-12-03T09:45:59.760921",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}