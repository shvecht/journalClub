{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa709aa8",
   "metadata": {
    "id": "6hIkSDQe8d3P",
    "papermill": {
     "duration": 0.002065,
     "end_time": "2026-01-03T13:11:58.602535",
     "exception": false,
     "start_time": "2026-01-03T13:11:58.600470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "PubMed Journal Article Fetcher for Google Colab\n",
    "This notebook fetches articles from specified journals for a given month using PubMed API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044baca1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-03T13:11:58.607844Z",
     "iopub.status.busy": "2026-01-03T13:11:58.607601Z",
     "iopub.status.idle": "2026-01-03T13:12:00.693057Z",
     "shell.execute_reply": "2026-01-03T13:12:00.692176Z"
    },
    "id": "rQ5dQa108bCT",
    "outputId": "668b9196-2fab-4691-d380-314ddecc5334",
    "papermill": {
     "duration": 2.088702,
     "end_time": "2026-01-03T13:12:00.693838",
     "exception": false,
     "start_time": "2026-01-03T13:11:58.605136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading biopython-1.86-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: pandas in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (2.3.3)\r\n",
      "Requirement already satisfied: requests in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (2.32.5)\r\n",
      "Requirement already satisfied: numpy in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from biopython) (2.4.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from pandas) (2025.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from requests) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from requests) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from requests) (2.6.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from requests) (2025.11.12)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Downloading biopython-1.86-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\r\n",
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: biopython\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed biopython-1.86\r\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "\n",
    "!pip install biopython pandas requests\n",
    "\n",
    "import os, pandas as pd, requests, time, warnings, json\n",
    "from Bio import Entrez\n",
    "from datetime import datetime, timedelta\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict, Optional\n",
    "import calendar\n",
    "# warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f54e7",
   "metadata": {
    "id": "u_TZagk_ARly",
    "papermill": {
     "duration": 0.00176,
     "end_time": "2026-01-03T13:12:00.697780",
     "exception": false,
     "start_time": "2026-01-03T13:12:00.696020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Instructions for use\n",
    "\n",
    "üìã INSTRUCTIONS:\n",
    "\n",
    "1. Update the EMAIL variable with your email address (required by NCBI)\n",
    "2. Adjust JOURNALS and LOOKBACK_DAYS to shape the search window\n",
    "3. Run the cells to execute main() and harvest only new PMIDs within the rolling window or the requested month\n",
    "\n",
    "‚ö†Ô∏è  IMPORTANT NOTES:\n",
    "\n",
    "- Use your real email address - it‚Äôs required by NCBI‚Äôs usage policy\n",
    "- Journal names should match PubMed‚Äôs format exactly\n",
    "- Large queries may take several minutes to complete\n",
    "- Be respectful of API rate limits\n",
    "- Previously harvested PMIDs are stored in data/ent_search/seen_pmids.json to avoid duplicates\n",
    "- When running via GitHub Actions, you can supply TARGET_MONTH (YYYY-MM) or YEAR + MONTH inputs to rerun a specific calendar month instead of the rolling LOOKBACK_DAYS window.\n",
    "\n",
    "üöÄ To start, run: main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7a44720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T13:12:00.702413Z",
     "iopub.status.busy": "2026-01-03T13:12:00.702114Z",
     "iopub.status.idle": "2026-01-03T13:12:00.706477Z",
     "shell.execute_reply": "2026-01-03T13:12:00.705822Z"
    },
    "id": "gzIuu6HoBSym",
    "papermill": {
     "duration": 0.007822,
     "end_time": "2026-01-03T13:12:00.707282",
     "exception": false,
     "start_time": "2026-01-03T13:12:00.699460",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EMAIL = os.getenv(\"UNPAYWALL_EMAIL\", \"\")\n",
    "\n",
    "JOURNALS = [\n",
    "    \"International Forum of Allergy & Rhinology\",\n",
    "    \"Rhinology\",\n",
    "    \"JAMA Otolaryngology‚ÄìHead & Neck Surgery\",\n",
    "    \"Otolaryngology‚ÄìHead and Neck Surgery\",\n",
    "    \"European Annals of Otorhinolaryngology‚ÄìHead and Neck Diseases\",\n",
    "    \"Journal of Voice\",\n",
    "    \"American Journal of Rhinology & Allergy\",\n",
    "    \"JARO ‚Äì Journal of the Association for Research in Otolaryngology\",\n",
    "    \"Journal of Otolaryngology‚ÄìHead & Neck Surgery\",\n",
    "    \"Laryngoscope\",\n",
    "    \"Auris Nasus Larynx\",\n",
    "    \"new england journal of medicine\",\n",
    "    \"JAMA\"\n",
    "]\n",
    "\n",
    "LOOKBACK_DAYS = int(os.getenv(\"LOOKBACK_DAYS\", \"30\"))\n",
    "TARGET_MONTH = os.getenv(\"TARGET_MONTH\", \"\").strip()\n",
    "TARGET_YEAR = os.getenv(\"TARGET_YEAR\", \"\").strip()\n",
    "TARGET_MONTH_NUMBER = os.getenv(\"TARGET_MONTH_NUMBER\", \"\").strip()\n",
    "OUTPUT_DIR = os.path.join(\"data\", \"ent_search\")\n",
    "\n",
    "ALLOW_EMPTY_HARVEST = os.getenv(\"ALLOW_EMPTY_HARVEST\", \"false\").lower() == \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4024ebfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T13:12:00.711924Z",
     "iopub.status.busy": "2026-01-03T13:12:00.711739Z",
     "iopub.status.idle": "2026-01-03T13:12:00.714474Z",
     "shell.execute_reply": "2026-01-03T13:12:00.713892Z"
    },
    "papermill": {
     "duration": 0.006088,
     "end_time": "2026-01-03T13:12:00.715312",
     "exception": false,
     "start_time": "2026-01-03T13:12:00.709224",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "TARGET_MONTH = \"2025-12\"\n",
    "TARGET_YEAR = \"\"\n",
    "TARGET_MONTH_NUMBER = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3690d365",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T13:12:00.719687Z",
     "iopub.status.busy": "2026-01-03T13:12:00.719491Z",
     "iopub.status.idle": "2026-01-03T13:12:00.722403Z",
     "shell.execute_reply": "2026-01-03T13:12:00.721534Z"
    },
    "id": "c5SuSLjlpAxh",
    "papermill": {
     "duration": 0.005851,
     "end_time": "2026-01-03T13:12:00.722902",
     "exception": false,
     "start_time": "2026-01-03T13:12:00.717051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "RUN_STARTED_AT = datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ff7a2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T13:12:00.727652Z",
     "iopub.status.busy": "2026-01-03T13:12:00.727465Z",
     "iopub.status.idle": "2026-01-03T13:12:00.750599Z",
     "shell.execute_reply": "2026-01-03T13:12:00.749908Z"
    },
    "id": "i_xUOYKgy-fl",
    "papermill": {
     "duration": 0.026615,
     "end_time": "2026-01-03T13:12:00.751287",
     "exception": false,
     "start_time": "2026-01-03T13:12:00.724672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "def load_seen_pmids(path: str) -> set:\n",
    "    \"\"\"Load a set of previously seen PMIDs from disk.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    return set(map(str, data))\n",
    "        except Exception as exc:\n",
    "            print(f\"‚ö†Ô∏è  Could not load seen PMIDs: {exc}\")\n",
    "    return set()\n",
    "\n",
    "\n",
    "def save_seen_pmids(path: str, pmids: set) -> None:\n",
    "    \"\"\"Persist a set of PMIDs to disk.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(sorted(pmids), f, indent=2)\n",
    "\n",
    "\n",
    "def compute_requested_window(target_month: str, target_year: str, target_month_number: str):\n",
    "    \"\"\"Return a (start_date, end_date) tuple for a requested calendar month.\"\"\"\n",
    "    target_month = (target_month or \"\").strip()\n",
    "    target_year = (target_year or \"\").strip()\n",
    "    target_month_number = (target_month_number or \"\").strip()\n",
    "\n",
    "    if target_month:\n",
    "        try:\n",
    "            start_date = datetime.strptime(target_month, \"%Y-%m\")\n",
    "        except ValueError as exc:\n",
    "            raise ValueError(f\"TARGET_MONTH must be YYYY-MM; received '{target_month}'\") from exc\n",
    "        end_day = calendar.monthrange(start_date.year, start_date.month)[1]\n",
    "        end_date = start_date.replace(day=end_day, hour=23, minute=59, second=59)\n",
    "        return start_date, end_date\n",
    "\n",
    "    if target_year and target_month_number:\n",
    "        try:\n",
    "            year_int = int(target_year)\n",
    "            month_int = int(target_month_number)\n",
    "            start_date = datetime(year_int, month_int, 1)\n",
    "        except ValueError as exc:\n",
    "            raise ValueError(\n",
    "                f\"TARGET_YEAR and TARGET_MONTH_NUMBER must form a valid date; received '{target_year}-{target_month_number}'\"\n",
    "            ) from exc\n",
    "        end_day = calendar.monthrange(year_int, month_int)[1]\n",
    "        end_date = start_date.replace(day=end_day, hour=23, minute=59, second=59)\n",
    "        return start_date, end_date\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "class PubMedFetcher:\n",
    "\n",
    "    def __init__(self, email: str):\n",
    "        \"\"\"Initialize with email for API requests\"\"\"\n",
    "        self.email = email\n",
    "        Entrez.email = email\n",
    "        # Be respectful to NCBI servers\n",
    "        self.request_delay = 0.34  # ~3 requests per second max\n",
    "\n",
    "    def search_articles(\n",
    "        self,\n",
    "        journals: List[str],\n",
    "        lookback_days: int,\n",
    "        start_date_override: Optional[datetime] = None,\n",
    "        end_date_override: Optional[datetime] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Search for articles in specified journals within a rolling or fixed window.\n",
    "\n",
    "        Args:\n",
    "            journals: List of journal names\n",
    "            lookback_days: Number of days to include in the rolling window\n",
    "            start_date_override: Explicit start date to use instead of the rolling window\n",
    "            end_date_override: Explicit end date to use instead of the rolling window\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (List of PubMed IDs, start_date_str, end_date_str)\n",
    "        \"\"\"\n",
    "        end_date = end_date_override or datetime.now()\n",
    "        start_date = start_date_override or end_date - timedelta(days=lookback_days)\n",
    "\n",
    "        start_date_str = start_date.strftime(\"%Y/%m/%d\")\n",
    "        end_date_str = end_date.strftime(\"%Y/%m/%d\")\n",
    "\n",
    "        journal_query = \" OR \".join([f'\"{journal}\"[Journal]' for journal in journals]) if journals else \"\"\n",
    "\n",
    "        query_parts = []\n",
    "        if journal_query:\n",
    "            query_parts.append(f\"({journal_query})\")\n",
    "        query_parts.append(f\"({start_date_str}[PDAT] : {end_date_str}[PDAT])\")\n",
    "\n",
    "        final_query = \" AND \".join(query_parts)\n",
    "\n",
    "        try:\n",
    "            print(f\"Querying PubMed with: {final_query}\")\n",
    "            handle = Entrez.esearch(db=\"pubmed\", term=final_query, datetype=\"pdat\", retmax=100000)\n",
    "            record = Entrez.read(handle)\n",
    "            handle.close()\n",
    "            time.sleep(self.request_delay)\n",
    "\n",
    "            search_results = record\n",
    "            id_list = search_results[\"IdList\"]\n",
    "            print(f\"Found {len(id_list)} articles\")\n",
    "            return id_list, start_date_str, end_date_str\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching PubMed: {e}\")\n",
    "            return [], start_date_str, end_date_str\n",
    "\n",
    "    def fetch_article_details(self, pmids: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetch detailed information for a list of PubMed IDs.\n",
    "\n",
    "        Args:\n",
    "            pmids: List of PubMed IDs\n",
    "\n",
    "        Returns:\n",
    "            List of article metadata dictionaries\n",
    "        \"\"\"\n",
    "        articles = []\n",
    "\n",
    "        try:\n",
    "            for pmid in pmids:\n",
    "                handle = Entrez.efetch(db=\"pubmed\", id=pmid, retmode=\"xml\")\n",
    "                time.sleep(self.request_delay)\n",
    "\n",
    "                record = Entrez.read(handle)\n",
    "                handle.close()\n",
    "\n",
    "                if not record or \"PubmedArticle\" not in record:\n",
    "                    print(f\"‚ö†Ô∏è  No article data found for PMID: {pmid}\")\n",
    "                    continue\n",
    "\n",
    "                article = record[\"PubmedArticle\"][0]\n",
    "\n",
    "                journal_info = article[\"MedlineCitation\"][\"Article\"][\"Journal\"]\n",
    "                journal = journal_info.get(\"Title\", \"Unknown Journal\")\n",
    "                title = article[\"MedlineCitation\"][\"Article\"].get(\"ArticleTitle\", \"No Title\")\n",
    "\n",
    "                abstract = \"\"\n",
    "                if \"Abstract\" in article[\"MedlineCitation\"][\"Article\"]:\n",
    "                    abstract_texts = article[\"MedlineCitation\"][\"Article\"].get(\"AbstractText\", [])\n",
    "                    abstract = \" \".join(abstract_texts)\n",
    "\n",
    "                authors = []\n",
    "                if \"AuthorList\" in article[\"MedlineCitation\"][\"Article\"]:\n",
    "                    for author in article[\"MedlineCitation\"][\"Article\"][\"AuthorList\"]:\n",
    "                        lastname = author.get(\"LastName\", \"\")\n",
    "                        firstname = author.get(\"ForeName\", \"\")\n",
    "                        fullname = f\"{firstname} {lastname}\".strip()\n",
    "                        if fullname:\n",
    "                            authors.append(fullname)\n",
    "\n",
    "                publication_date = \"Unknown\"\n",
    "                pub_date = journal_info.get(\"JournalIssue\", {}).get(\"PubDate\", {})\n",
    "                year = pub_date.get(\"Year\", \"\")\n",
    "                month = pub_date.get(\"Month\", \"\")\n",
    "                day = pub_date.get(\"Day\", \"\")\n",
    "                if year:\n",
    "                    publication_date = year\n",
    "                    if month:\n",
    "                        publication_date += f\"-{month}\"\n",
    "                    if day:\n",
    "                        publication_date += f\"-{day}\"\n",
    "\n",
    "                doi = \"\"\n",
    "                for article_id in article[\"PubmedData\"].get(\"ArticleIdList\", []):\n",
    "                    if article_id.attributes.get(\"IdType\") == \"doi\":\n",
    "                        doi = str(article_id)\n",
    "                        break\n",
    "\n",
    "                volume = journal_info.get(\"JournalIssue\", {}).get(\"Volume\", \"\")\n",
    "                issue = journal_info.get(\"JournalIssue\", {}).get(\"Issue\", \"\")\n",
    "                pages = article[\"MedlineCitation\"][\"Article\"].get(\"Pagination\", {}).get(\"MedlinePgn\", \"\")\n",
    "\n",
    "                articles.append({\n",
    "                    \"PMID\": pmid,\n",
    "                    \"Title\": title,\n",
    "                    \"Journal\": journal,\n",
    "                    \"Authors\": \"; \".join(authors),\n",
    "                    \"PublicationDate\": publication_date,\n",
    "                    \"Volume\": volume,\n",
    "                    \"Issue\": issue,\n",
    "                    \"Pages\": pages,\n",
    "                    \"DOI\": doi,\n",
    "                    \"Abstract\": abstract[:500] + \"...\" if len(abstract) > 500 else abstract  # Truncate long abstracts\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing article: {e}\")\n",
    "            return None\n",
    "\n",
    "        return articles\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(\"=== PubMed Journal Article Fetcher ===\")\n",
    "\n",
    "    print(f\"Email: {EMAIL}\")\n",
    "    if JOURNALS:\n",
    "        print(f\"Journals: {', '.join(JOURNALS)}\")\n",
    "    else:\n",
    "        print(\"No journal filter configured; searching across all journals.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    requested_window = compute_requested_window(TARGET_MONTH, TARGET_YEAR, TARGET_MONTH_NUMBER)\n",
    "    if requested_window:\n",
    "        start_dt, end_dt = requested_window\n",
    "        print(\n",
    "            f\"Requested month window: {start_dt.strftime('%Y-%m-%d')} to {end_dt.strftime('%Y-%m-%d')} (from TARGET_MONTH/TARGET_YEAR+TARGET_MONTH_NUMBER)\"\n",
    "        )\n",
    "    else:\n",
    "        end_dt = datetime.now()\n",
    "        start_dt = end_dt - timedelta(days=LOOKBACK_DAYS)\n",
    "        print(f\"Rolling window: last {LOOKBACK_DAYS} days ({start_dt.strftime('%Y-%m-%d')} to {end_dt.strftime('%Y-%m-%d')})\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Validate email\n",
    "    if EMAIL == \"your.email@example.com\":\n",
    "        print(\"‚ö†Ô∏è  Please update the EMAIL variable with your actual email address!\")\n",
    "        print(\"This is required by NCBI's API usage policy.\")\n",
    "        return\n",
    "\n",
    "    # Initialize fetcher\n",
    "    fetcher = PubMedFetcher(EMAIL)\n",
    "\n",
    "    # Search for articles within the requested window\n",
    "    print(\"üîç Searching for articles...\")\n",
    "    pmid_list, start_date, end_date = fetcher.search_articles(\n",
    "        JOURNALS,\n",
    "        LOOKBACK_DAYS,\n",
    "        start_dt,\n",
    "        end_dt,\n",
    "    )\n",
    "\n",
    "    if not pmid_list:\n",
    "        print(\"‚ùå No articles found matching the criteria.\")\n",
    "        print(f\"Summary: discovered {len(pmid_list)} PMIDs between {start_date} and {end_date}.\")\n",
    "        if ALLOW_EMPTY_HARVEST:\n",
    "            print(\"‚ö†Ô∏è  Empty harvest allowed via ALLOW_EMPTY_HARVEST flag; exiting without failure.\")\n",
    "            return\n",
    "        raise RuntimeError(\"No articles found for the configured search window.\")\n",
    "\n",
    "    seen_pmids_path = os.path.join(OUTPUT_DIR, \"seen_pmids.json\")\n",
    "    seen_pmids = load_seen_pmids(seen_pmids_path)\n",
    "    if seen_pmids:\n",
    "        print(f\"Loaded {len(seen_pmids)} previously harvested PMIDs.\")\n",
    "\n",
    "    new_pmids = [pmid for pmid in pmid_list if pmid not in seen_pmids]\n",
    "    print(f\"PMIDs to fetch after filtering seen set: {len(new_pmids)}\")\n",
    "\n",
    "    if not new_pmids:\n",
    "        print(\"‚ö†Ô∏è  No new PMIDs to process within this window.\")\n",
    "        print(f\"Summary: discovered {len(pmid_list)} PMIDs, seen {len(seen_pmids)}, new {len(new_pmids)} between {start_date} and {end_date}.\")\n",
    "        if ALLOW_EMPTY_HARVEST:\n",
    "            print(\"‚ö†Ô∏è  Empty harvest allowed via ALLOW_EMPTY_HARVEST flag; exiting without failure.\")\n",
    "            return\n",
    "        raise RuntimeError(\"No new PMIDs to process after filtering seen set.\")\n",
    "\n",
    "    # Fetch article details\n",
    "    print(f\"üìñ Fetching details for {len(new_pmids)} new articles...\")\n",
    "    articles = fetcher.fetch_article_details(new_pmids)\n",
    "\n",
    "    if not articles:\n",
    "        print(\"‚ùå Failed to fetch article details.\")\n",
    "        return\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"‚úÖ Successfully retrieved {len(articles)} articles!\")\n",
    "    print(f\"Columns: {', '.join(df.columns.tolist())}\")\n",
    "\n",
    "    # Show first few rows\n",
    "    print(f\"First 5 articles:\")\n",
    "    print(df.head().to_string(max_colwidth=50))\n",
    "\n",
    "    # Save outputs\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    window_label = f\"{start_date.replace('/', '-')}_to_{end_date.replace('/', '-')}\"\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"ent_raw_results_{window_label}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    updated_seen_pmids = seen_pmids | set(new_pmids)\n",
    "    save_seen_pmids(seen_pmids_path, updated_seen_pmids)\n",
    "    print(f\"Updated seen PMIDs saved to: {seen_pmids_path}\")\n",
    "\n",
    "    json_path = os.path.join(OUTPUT_DIR, \"ent_all_results.json\")\n",
    "    df.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
    "\n",
    "    # Display summary statistics\n",
    "    print(f\"üìä Summary:\")\n",
    "    print(f\"Total new articles: {len(articles)}\")\n",
    "    print(f\"Unique journals: {df['Journal'].nunique()}\")\n",
    "    print(f\"Articles per journal:\")\n",
    "    journal_counts = df['Journal'].value_counts()\n",
    "    for journal, count in journal_counts.head(10).items():\n",
    "        print(f\"  ‚Ä¢ {journal}: {count}\")\n",
    "\n",
    "    print(f\"üíæ Raw CSV saved to: {csv_path}\")\n",
    "    print(f\"üíæ ENT results JSON saved to: {json_path}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98d1f567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T13:12:00.755902Z",
     "iopub.status.busy": "2026-01-03T13:12:00.755740Z",
     "iopub.status.idle": "2026-01-03T13:12:00.759706Z",
     "shell.execute_reply": "2026-01-03T13:12:00.759028Z"
    },
    "papermill": {
     "duration": 0.007015,
     "end_time": "2026-01-03T13:12:00.760191",
     "exception": false,
     "start_time": "2026-01-03T13:12:00.753176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dry run disabled. Set DRY_RUN_PREVIEW=true to log a quick query.\n"
     ]
    }
   ],
   "source": [
    "# Optional dry-run preview for the last 30 days\n",
    "# Set DRY_RUN_PREVIEW=true in the environment to exercise this without editing code.\n",
    "DRY_RUN_PREVIEW = os.getenv(\"DRY_RUN_PREVIEW\", \"false\").lower() == \"true\"\n",
    "\n",
    "if DRY_RUN_PREVIEW:\n",
    "    if not EMAIL:\n",
    "        raise ValueError(\"EMAIL must be configured to run the dry-run search.\")\n",
    "    dry_run_end = datetime.now()\n",
    "    dry_run_start = dry_run_end - timedelta(days=30)\n",
    "    print(f\"Dry-run window: {dry_run_start.strftime('%Y-%m-%d')} to {dry_run_end.strftime('%Y-%m-%d')}\")\n",
    "    print(\"Running dry-run search...\")\n",
    "    fetcher = PubMedFetcher(EMAIL)\n",
    "    pmid_list, start_date, end_date = fetcher.search_articles(\n",
    "        JOURNALS,\n",
    "        LOOKBACK_DAYS,\n",
    "        dry_run_start,\n",
    "        dry_run_end,\n",
    "    )\n",
    "    print(f\"Dry-run returned {len(pmid_list)} PMIDs.\")\n",
    "    print(\"Sample PMIDs:\", pmid_list[:10])\n",
    "else:\n",
    "    print(\"Dry run disabled. Set DRY_RUN_PREVIEW=true to log a quick query.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38670212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T13:12:00.764793Z",
     "iopub.status.busy": "2026-01-03T13:12:00.764641Z",
     "iopub.status.idle": "2026-01-03T13:17:51.796364Z",
     "shell.execute_reply": "2026-01-03T13:17:51.795630Z"
    },
    "id": "xAVY_hYgpk5c",
    "papermill": {
     "duration": 351.036247,
     "end_time": "2026-01-03T13:17:51.798315",
     "exception": false,
     "start_time": "2026-01-03T13:12:00.762068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PubMed Journal Article Fetcher ===\n",
      "Email: shvecht@gmail.com\n",
      "Journals: International Forum of Allergy & Rhinology, Rhinology, JAMA Otolaryngology‚ÄìHead & Neck Surgery, Otolaryngology‚ÄìHead and Neck Surgery, European Annals of Otorhinolaryngology‚ÄìHead and Neck Diseases, Journal of Voice, American Journal of Rhinology & Allergy, JARO ‚Äì Journal of the Association for Research in Otolaryngology, Journal of Otolaryngology‚ÄìHead & Neck Surgery, Laryngoscope, Auris Nasus Larynx, new england journal of medicine, JAMA\n",
      "--------------------------------------------------\n",
      "Requested month window: 2025-12-01 to 2025-12-31 (from TARGET_MONTH/TARGET_YEAR+TARGET_MONTH_NUMBER)\n",
      "--------------------------------------------------\n",
      "üîç Searching for articles...\n",
      "Querying PubMed with: (\"International Forum of Allergy & Rhinology\"[Journal] OR \"Rhinology\"[Journal] OR \"JAMA Otolaryngology‚ÄìHead & Neck Surgery\"[Journal] OR \"Otolaryngology‚ÄìHead and Neck Surgery\"[Journal] OR \"European Annals of Otorhinolaryngology‚ÄìHead and Neck Diseases\"[Journal] OR \"Journal of Voice\"[Journal] OR \"American Journal of Rhinology & Allergy\"[Journal] OR \"JARO ‚Äì Journal of the Association for Research in Otolaryngology\"[Journal] OR \"Journal of Otolaryngology‚ÄìHead & Neck Surgery\"[Journal] OR \"Laryngoscope\"[Journal] OR \"Auris Nasus Larynx\"[Journal] OR \"new england journal of medicine\"[Journal] OR \"JAMA\"[Journal]) AND (2025/12/01[PDAT] : 2025/12/31[PDAT])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 533 articles\n",
      "PMIDs to fetch after filtering seen set: 533\n",
      "üìñ Fetching details for 533 new articles...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully retrieved 533 articles!\n",
      "Columns: PMID, Title, Journal, Authors, PublicationDate, Volume, Issue, Pages, DOI, Abstract\n",
      "First 5 articles:\n",
      "       PMID                                              Title                                            Journal                                            Authors PublicationDate Volume Issue  Pages                          DOI Abstract\n",
      "0  41478836  Guidelines of the French Society of ENT (short...  European annals of otorhinolaryngology, head a...  X Dubernard; J Ortega Solis; M-J Fraysse; S Tr...     2025-Dec-31                      10.1016/j.anorl.2025.12.001         \n",
      "1  41472338  Continuous Sleep Monitoring Using the Withings...                                   The Laryngoscope  Praneet C Kaki; Maya Childs; Randy Calotti; Ja...     2025-Dec-30                               10.1002/lary.70352         \n",
      "2  41469328  In Reference to Bilateral Sudden or Chronic He...                                   The Laryngoscope                                    Josef Finsterer     2025-Dec-30                               10.1002/lary.70348         \n",
      "3  41469321  In Response to Bilateral Sudden on Chronic Hea...                                   The Laryngoscope            Chin-Nung Liu; Hong-Yu Yan; Chen-Chi Wu     2025-Dec-30                               10.1002/lary.70346         \n",
      "4  41468859  Validity and reliability of Bahasa Malaysia ea...                               Auris, nasus, larynx  Jia Ji Ng; Hui Yan Ong; Ziyad Al Harrasi; Nik ...     2025-Dec-29     53     1  91-98    10.1016/j.anl.2025.12.002         \n",
      "Updated seen PMIDs saved to: data/ent_search/seen_pmids.json\n",
      "üìä Summary:\n",
      "Total new articles: 533\n",
      "Unique journals: 8\n",
      "Articles per journal:\n",
      "  ‚Ä¢ JAMA: 242\n",
      "  ‚Ä¢ The Laryngoscope: 148\n",
      "  ‚Ä¢ Auris, nasus, larynx: 40\n",
      "  ‚Ä¢ International forum of allergy & rhinology: 34\n",
      "  ‚Ä¢ JAMA otolaryngology-- head & neck surgery: 29\n",
      "  ‚Ä¢ Rhinology: 24\n",
      "  ‚Ä¢ European annals of otorhinolaryngology, head and neck diseases: 9\n",
      "  ‚Ä¢ American journal of rhinology & allergy: 7\n",
      "üíæ Raw CSV saved to: data/ent_search/ent_raw_results_2025-12-01_to_2025-12-31.csv\n",
      "üíæ ENT results JSON saved to: data/ent_search/ent_all_results.json\n",
      "=== DataFrame Summary ===\n",
      "            PMID                              Title Journal Authors  \\\n",
      "count        533                                533     533     533   \n",
      "unique       533                                521       8     457   \n",
      "top     41478836  A Review of Hemorrhoidal Disease.    JAMA           \n",
      "freq           1                                  3     242      31   \n",
      "\n",
      "       PublicationDate Volume Issue Pages                          DOI  \\\n",
      "count              533    533   533   533                          533   \n",
      "unique              29     10     7   292                          533   \n",
      "top           2025-Dec                     10.1016/j.anorl.2025.12.001   \n",
      "freq               122    253   253   232                            1   \n",
      "\n",
      "       Abstract  \n",
      "count       533  \n",
      "unique        1  \n",
      "top              \n",
      "freq        533  \n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "df = main()\n",
    "if df is not None:\n",
    "    print(\"=== DataFrame Summary ===\")\n",
    "    print(df.describe(include='all'))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 354.534345,
   "end_time": "2026-01-03T13:17:52.015458",
   "environment_variables": {},
   "exception": null,
   "input_path": "articlesRetrieval.ipynb",
   "output_path": "articlesRetrieval.out.ipynb",
   "parameters": {
    "TARGET_MONTH": "2025-12",
    "TARGET_MONTH_NUMBER": "",
    "TARGET_YEAR": ""
   },
   "start_time": "2026-01-03T13:11:57.481113",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}